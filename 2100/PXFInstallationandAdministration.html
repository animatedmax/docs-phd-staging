<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">

  
  <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">

  
  <title>PXF Installation and Administration | Pivotal HD Docs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <link href="stylesheets/master.css" media="screen,print" rel="stylesheet" type="text/css">
  <link href="stylesheets/print.css" media="print" rel="stylesheet" type="text/css">
  <link href="http://www.pivotal.io/misc/favicon.ico" rel="shortcut icon">
  <script src="javascripts/all.js" type="text/javascript"></script>
  
  <script type="text/javascript">
    if (window.location.host === 'docs.pivotal.io') {
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-39702075-1']);
      _gaq.push(['_setDomainName', 'pivotal.io']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    }
  </script>
  
   <link href="stylesheets/site-conf.css" media="screen,print" rel="stylesheet" type="text/css"> 
   
   
    <link href="stylesheets/dcaccordion.css" rel="stylesheet" type="text/css">
    
   
</head>

<body class="pivotalhd pivotalhd_getstarted pivotalhd_getstarted_phd-docs has-subnav">

<div class="viewport">
<div class="wrap">
  <script src="//use.typekit.net/clb0qji.js" type="text/javascript"></script>
  <script type="text/javascript">
      try {
          Typekit.load();
      } catch (e) {
      }
  </script>
  <script type="text/javascript">
      document.domain = "pivotal.io";
  </script>

  <script type="text/javascript">
    WebFontConfig = {
      google: { families: [ 'Source+Sans+Pro:300italic,400italic,300,400,600:latin' ] }
    };
    (function() {
      var wf = document.createElement('script');
      wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
        '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
      wf.type = 'text/javascript';
      wf.async = 'true';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(wf, s);
    })();
  </script>
   
    <header class="header header-layout">
      <h1 class="logo">
        <a href="index.html">Pivotal HD Documentation</a> v2.1.0
      </h1>
      <div class="header-links js-bar-links">
        <div class="btn-menu" data-behavior="MenuMobile"></div>
        <div class="header-item">
          <a href="http://docs.pivotal.io">Docs Home</a>
        </div>
        <div class="header-item">
          <a href="http://support.pivotal.io" target="_blank">Support</a>
        </div>
        <div class="header-item searchbar js-searchbar">
          <a class="search-icon" data-behavior="Search"></a>
          <div class="search-input">
            
            <div class="search-input-inner" id="docs-search">
                <div class="gcse-search"></div>
            </div>
          </div>
        </div>
      </div>
    </header>
    
    <div class="container">
    
	
	<div id="sub-nav" class="nav-container">

         <p class="productname"><a href="PivotalHD.html">Pivotal HD</a></p>
         <ul>
            <li>
               <a href="PHDEnterprise2.1.0ReleaseNotes.html">PHD Enterprise 2.1.0 Release Notes</a>
            </li>
            <li>
               <a href="PHDInstallationandAdministration.html">PHD Installation and Administration</a>
               <ul>
                  <li>
                     <a href="OverviewofPHD.html">Overview of PHD</a>
                  </li>
                  <li>
                     <a href="InstallationOverview.html">Installation Overview</a>
                     <ul>
                        <li>
                           <a href="CommandLineInstallationFeatures.html">Command Line Installation Features</a>
                        </li>
                        <li>
                           <a href="DeploymentOptions.html">Deployment Options</a>
                        </li>
                        <li>
                           <a href="PlanningyourPHDClusterDeployment.html">Planning your PHD Cluster Deployment</a>
                        </li>
                        <li>
                           <a href="BestPracticesforSelectingHardware.html">Best Practices for Selecting Hardware</a>
                        </li>
                        <li>
                           <a href="BestPracticesforDeployingHadoopServices.html">Best Practices for Deploying Hadoop Services</a>
                        </li>
                        <li>
                           <a href="HighAvailabilityBestPractices.html">High Availability Best Practices</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="PHDInstallationPrerequisites.html">PHD Installation Prerequisites</a>
                     <ul>
                        <li>
                           <a href="BeforeYouBeginInstallingPHD.html">Before You Begin Installing PHD</a>
                        </li>
                        <li>
                           <a href="PHDPrerequisiteChecklist.html">PHD Prerequisite Checklist</a>
                        </li>
                        <li>
                           <a href="PHD-DNSLookup.html">PHD - DNS Lookup</a>
                        </li>
                        <li>
                           <a href="PHD-JAVAJDK.html">PHD - JAVA JDK</a>
                        </li>
                        <li>
                           <a href="PHD-VerifyPackageAccessibility.html">PHD - Verify Package Accessibility</a>
                        </li>
                        <li>
                           <a href="PHD-TurnOffiptables.html">PHD - Turn Off iptables</a>
                        </li>
                        <li>
                           <a href="PHD-DisableSELinux.html">PHD - Disable SELinux</a>
                        </li>
                        <li>
                           <a href="EPELYumRepository.html">EPEL Yum Repository</a>
                        </li>
                        <li>
                           <a href="SudoConfigurationFiles.html">Sudo Configuration Files</a>
                        </li>
                        <li>
                           <a href="FQDN.html">FQDN</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="InstallingPHDUsingtheCLI.html">Installing PHD Using the CLI</a>
                     <ul>
                        <li>
                           <a href="PHDInstallationChecklist.html">PHD Installation Checklist</a>
                        </li>
                        <li>
                           <a href="InstallingPivotalCommandCenter.html">Installing Pivotal Command Center</a>
                        </li>
                        <li>
                           <a href="ConfiguringKerberosandLDAP.html">Configuring Kerberos and LDAP</a>
                        </li>
                        <li>
                           <a href="PHD-ImportingthePackages.html">PHD - Importing the Packages</a>
                        </li>
                        <li>
                           <a href="EditingtheClusterConfigurationFiles.html">Editing the Cluster Configuration Files</a>
                        </li>
                        <li>
                           <a href="EditingtheHAWQConfigurationFile.html">Editing the HAWQ Configuration File</a>
                        </li>
                        <li>
                           <a href="ForPXFwithGemFireXD.html">For PXF with GemFire XD</a>
                        </li>
                        <li>
                           <a href="DeployingtheCluster.html">Deploying the Cluster</a>
                        </li>
                        <li>
                           <a href="StartingtheCluster.html">Starting the Cluster</a>
                        </li>
                        <li>
                           <a href="InitializingandStartingHAWQ.html">Initializing and Starting HAWQ</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="PHDPost-Installation.html">PHD Post-Installation</a>
                     <ul>
                        <li>
                           <a href="VerifyingPHDServiceStatus.html">Verifying PHD Service Status</a>
                        </li>
                        <li>
                           <a href="RunningPHDSamplePrograms.html">Running PHD Sample Programs</a>
                        </li>
                        <li>
                           <a href="Post-InstallationReferenceInformation.html">Post-Installation Reference Information</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="PHDUpgradePrerequisites.html">PHD Upgrade Prerequisites</a>
                     <ul>
                        <li>
                           <a href="UpgradePrerequisiteChecklist.html">Upgrade Prerequisite Checklist</a>
                        </li>
                        <li>
                           <a href="FileLocationsandBackups.html">File Locations and Backups</a>
                        </li>
                        <li>
                           <a href="VerifyJavaJDK.html">Verify Java JDK</a>
                        </li>
                        <li>
                           <a href="CompactHBaseTables.html">Compact HBase Tables</a>
                        </li>
                        <li>
                           <a href="SudoConfigurationFile.html">Sudo Configuration File</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="UpgradingPHD2.0.xto2.1.0.html">Upgrading PHD 2.0.x to 2.1.0</a>
                     <ul>
                        <li>
                           <a href="PHDUpgradeChecklist-2.0.xto2.1.0.html">PHD Upgrade Checklist - 2.0.x to 2.1.0</a>
                        </li>
                        <li>
                           <a href="UpgradeInstructions-2.0.xto2.1.0.html">Upgrade Instructions - 2.0.x to 2.1.0</a>
                        </li>
                        <li>
                           <a href="MovingHAWQFilespacetoHA-enabledHDFS-2.0.xto2.1.0.html">Moving HAWQ Filespace to HA-enabled HDFS - 2.0.x to 2.1.0</a>
                        </li>
                        <li>
                           <a href="UpgradeReferenceInformation-2.0.xto2.1.0.html">Upgrade Reference Information - 2.0.x to 2.1.0</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="UpgradingPHDfrom1.1.1to2.1.0.html">Upgrading PHD from 1.1.1 to 2.1.0</a>
                     <ul>
                        <li>
                           <a href="UpgradeChecklist-1.1.1to2.1.0.html">Upgrade Checklist - 1.1.1 to 2.1.0</a>
                        </li>
                        <li>
                           <a href="UpgradeInstructions-1.1.1to2.1.0.html">Upgrade Instructions - 1.1.1 to 2.1.0</a>
                        </li>
                        <li>
                           <a href="MovingHAWQFilespacetoHA-enabledHDFS-1.1.1to2.1.0.html">Moving HAWQ Filespace to HA-enabled HDFS - 1.1.1 to 2.1.0</a>
                        </li>
                        <li>
                           <a href="UpgradeReferenceInformation-1.1.1to2.1.0.html">Upgrade Reference Information - 1.1.1 to 2.1.0</a>
                        </li>
                        <li>
                           <a href="DisablingSecurityona1.1.1Cluster.html">Disabling Security on a 1.1.1 Cluster</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="AdministeringPHDUsingtheCLI.html">Administering PHD Using the CLI</a>
                     <ul>
                        <li>
                           <a href="ManagingaPHDCluster.html">Managing a PHD Cluster</a>
                           <ul>
                              <li>
                                 <a href="StartingaCluster.html">Starting a Cluster</a>
                              </li>
                              <li>
                                 <a href="StoppingaCluster.html">Stopping a Cluster</a>
                              </li>
                              <li>
                                 <a href="RestartingaCluster.html">Restarting a Cluster</a>
                              </li>
                              <li>
                                 <a href="ReconfiguringaCluster.html">Reconfiguring a Cluster</a>
                              </li>
                              <li>
                                 <a href="AddingRemovingServices.html">Adding/Removing Services</a>
                              </li>
                              <li>
                                 <a href="AddingHoststoaCluster.html">Adding Hosts to a Cluster</a>
                              </li>
                              <li>
                                 <a href="RetrievingInformationaboutaDeployedCluster.html">Retrieving Information about a Deployed Cluster</a>
                              </li>
                              <li>
                                 <a href="ListingClusters.html">Listing Clusters</a>
                              </li>
                              <li>
                                 <a href="ExpandingaCluster.html">Expanding a Cluster</a>
                              </li>
                              <li>
                                 <a href="ShrinkingaCluster.html">Shrinking a Cluster</a>
                              </li>
                              <li>
                                 <a href="DecommissioningSlaveNodes.html">Decommissioning Slave Nodes</a>
                                 <ul>
                                    <li>
                                       <a href="DecommissionNodesOverview.html">Decommission Nodes Overview</a>
                                    </li>
                                    <li>
                                       <a href="DecommissioningtheDataNode.html">Decommissioning the Data Node</a>
                                    </li>
                                    <li>
                                       <a href="DecommissioningtheYARNNodeManager.html">Decommissioning the YARN NodeManager</a>
                                    </li>
                                    <li>
                                       <a href="ShuttingDowntheSlaveNode.html">Shutting Down the Slave Node</a>
                                    </li>
                                    <li>
                                       <a href="ReplacingtheSlaveNode.html">Replacing the Slave Node</a>
                                    </li>
                                    <li>
                                       <a href="ReplacingtheSlaveNodeDisk.html">Replacing the Slave Node Disk</a>
                                    </li>
                                 </ul>
                              </li>
                              <li>
                                 <a href="HighAvailability.html">High Availability</a>
                                 <ul>
                                    <li>
                                       <a href="DisablingHighAvailability.html">Disabling High Availability</a>
                                    </li>
                                    <li>
                                       <a href="EnablingRe-enablingHighAvailability.html">Enabling/Re-enabling High Availability</a>
                                    </li>
                                    <li>
                                       <a href="HighAvailabilityCommandReference.html">High Availability Command Reference</a>
                                    </li>
                                 </ul>
                              </li>
                              <li>
                                 <a href="SecurityKerberosAuthentication.html">Security/Kerberos Authentication</a>
                                 <ul>
                                    <li>
                                       <a href="EnablingKerberosAuthentication.html">Enabling Kerberos Authentication</a>
                                    </li>
                                    <li>
                                       <a href="DisablingKerberosAuthentication.html">Disabling Kerberos Authentication</a>
                                    </li>
                                    <li>
                                       <a href="SecureModeCommands.html">Secure Mode Commands</a>
                                    </li>
                                 </ul>
                              </li>
                              <li>
                                 <a href="UninstallingaCluster.html">Uninstalling a Cluster</a>
                              </li>
                           </ul>
                        </li>
                        <li>
                           <a href="ManagingHAWQ.html">Managing HAWQ</a>
                           <ul>
                              <li>
                                 <a href="InitializingHAWQ.html">Initializing HAWQ</a>
                              </li>
                              <li>
                                 <a href="StartingHAWQ.html">Starting HAWQ</a>
                              </li>
                              <li>
                                 <a href="StoppingHAWQ.html">Stopping HAWQ</a>
                              </li>
                              <li>
                                 <a href="ModifyingHAWQUserConfiguration.html">Modifying HAWQ User Configuration</a>
                              </li>
                              <li>
                                 <a href="ExpandingHAWQ.html">Expanding HAWQ</a>
                              </li>
                           </ul>
                        </li>
                        <li>
                           <a href="ManagingPHDRolesandHosts.html">Managing PHD Roles and Hosts</a>
                        </li>
                        <li>
                           <a href="PHDServicesReference.html">PHD Services Reference</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="PHDFAQ-FrequentlyAskedQuestions.html">PHD FAQ - Frequently Asked Questions</a>
                  </li>
                  <li>
                     <a href="PHDTroubleshooting.html">PHD Troubleshooting</a>
                  </li>
                  <li>
                     <a href="PHDRESTAPIs.html">PHD REST APIs</a>
                     <ul>
                        <li>
                           <a href="SwaggerwithOAuth.html">Swagger with OAuth</a>
                        </li>
                        <li>
                           <a href="ListofPHDRESTAPIs.html">List of PHD REST APIs</a>
                        </li>
                     </ul>
                  </li>
               </ul>
            </li>
            <li>
               <a href="StackandToolsReference.html">Stack and Tools Reference</a>
               <ul>
                  <li>
                     <a href="OverviewofApacheStackandPivotalComponents.html">Overview of Apache Stack and Pivotal Components</a>
                  </li>
                  <li>
                     <a href="ManuallyInstallingandUsingPivotalHD2.1Stack.html">Manually Installing and Using Pivotal HD 2.1 Stack</a>
                     <ul>
                        <li>
                           <a href="DistributionContents.html">Distribution Contents</a>
                        </li>
                        <li>
                           <a href="ApacheConfigurationReference.html">Apache Configuration Reference</a>
                        </li>
                        <li>
                           <a href="AccessingPHD2.1.html">Accessing PHD 2.1</a>
                        </li>
                        <li>
                           <a href="HadoopHDFS.html">Hadoop HDFS</a>
                        </li>
                        <li>
                           <a href="HadoopYARN.html">Hadoop YARN</a>
                        </li>
                        <li>
                           <a href="HadoopPseudo-distributedConfiguration.html">Hadoop Pseudo-distributed Configuration</a>
                        </li>
                        <li>
                           <a href="Zookeeper.html">Zookeeper</a>
                        </li>
                        <li>
                           <a href="HBase.html">HBase</a>
                        </li>
                        <li>
                           <a href="Hive.html">Hive</a>
                        </li>
                        <li>
                           <a href="HCatalog.html">HCatalog</a>
                        </li>
                        <li>
                           <a href="Pig.html">Pig</a>
                        </li>
                        <li>
                           <a href="Mahout.html">Mahout</a>
                        </li>
                        <li>
                           <a href="Flume.html">Flume</a>
                        </li>
                        <li>
                           <a href="Sqoop.html">Sqoop</a>
                        </li>
                        <li>
                           <a href="Oozie.html">Oozie</a>
                        </li>
                        <li>
                           <a href="GraphLab.html">GraphLab</a>
                        </li>
                        <li>
                           <a href="Hamster.html">Hamster</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="ManuallyUpgradingPivotalHDStacktoPHD2.1.html">Manually Upgrading Pivotal HD Stack to PHD 2.1</a>
                  </li>
                  <li>
                     <a href="PivotalHadoopEnhancements.html">Pivotal Hadoop Enhancements</a>
                  </li>
                  <li>
                     <a href="Security.html">Security</a>
                     <ul>
                        <li>
                           <a href="SecurityOverview.html">Security Overview</a>
                        </li>
                        <li>
                           <a href="KerberosSetup.html">Kerberos Setup</a>
                        </li>
                        <li>
                           <a href="LDAPSetup.html">LDAP Setup</a>
                        </li>
                        <li>
                           <a href="ConfiguringKerberosforHDFSandYARNMapReduce.html">Configuring Kerberos for HDFS and YARN (MapReduce)</a>
                        </li>
                        <li>
                           <a href="ConfiguringKerberosforHDFSHighAvailability.html">Configuring Kerberos for HDFS High Availability</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureZookeeper.html">Configuring Secure Zookeeper</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureHBase.html">Configuring Secure HBase</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureHive.html">Configuring Secure Hive</a>
                        </li>
                        <li>
                           <a href="ConfiguringHCatalogWebHCatonSecureHive.html">Configuring HCatalog (WebHCat) on Secure Hive</a>
                        </li>
                        <li>
                           <a href="ConfiguringHAWQonSecureHDFS.html">Configuring HAWQ on Secure HDFS</a>
                        </li>
                        <li>
                           <a href="EnablingAuditing.html">Enabling Auditing</a>
                        </li>
                        <li>
                           <a href="SecureWebAccess.html">Secure Web Access</a>
                        </li>
                        <li>
                           <a href="SecureWebAccessviaHttpFS.html">Secure Web Access via HttpFS</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureFlume.html">Configuring Secure Flume</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureOozie.html">Configuring Secure Oozie</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureSqoop.html">Configuring Secure Sqoop</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecurePig.html">Configuring Secure Pig</a>
                        </li>
                        <li>
                           <a href="ConfiguringSecureMahout.html">Configuring Secure Mahout</a>
                        </li>
                        <li>
                           <a href="Security-Troubleshooting.html">Security - Troubleshooting</a>
                        </li>
                     </ul>
                  </li>
               </ul>
            </li>
         </ul>

         <p class="productname"><a href="PivotalCommandCenter.html">Pivotal Command Center</a></p>
         <ul>
            <li>
               <a href="PCC2.3.0ReleaseNotes.html">PCC 2.3.0 Release Notes</a>
            </li>
            <li>
               <a href="PCCUserGuide.html">PCC User Guide</a>
               <ul>
                  <li>
                     <a href="PCCOverview.html">PCC Overview</a>
                  </li>
                  <li>
                     <a href="PCCInstallationPrerequisites.html">PCC Installation Prerequisites</a>
                     <ul>
                        <li>
                           <a href="BeforeYouBeginInstallingPCC.html">Before You Begin Installing PCC</a>
                        </li>
                        <li>
                           <a href="PCCPrerequisiteChecklist.html">PCC Prerequisite Checklist</a>
                        </li>
                        <li>
                           <a href="PCC-DNSLookup.html">PCC - DNS Lookup</a>
                        </li>
                        <li>
                           <a href="PCC-JAVAJDK.html">PCC - JAVA JDK</a>
                        </li>
                        <li>
                           <a href="PCC-VerifyPackageAccessibility.html">PCC - Verify Package Accessibility</a>
                        </li>
                        <li>
                           <a href="PCC-TurnOffiptables.html">PCC - Turn Off iptables</a>
                        </li>
                        <li>
                           <a href="PCC-DisableSELinux.html">PCC - Disable SELinux</a>
                        </li>
                        <li>
                           <a href="PCC-EPELYumRepository.html">PCC - EPEL Yum Repository</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="InstallationInstructions.html">Installation Instructions</a>
                     <ul>
                        <li>
                           <a href="SupportedPlatformsandBrowsers.html">Supported Platforms and Browsers</a>
                        </li>
                        <li>
                           <a href="PCCInstallationChecklist.html">PCC Installation Checklist</a>
                        </li>
                        <li>
                           <a href="InstallPCC.html">Install PCC</a>
                        </li>
                        <li>
                           <a href="PCC-ImportingthePackages.html">PCC - Importing the Packages</a>
                        </li>
                        <li>
                           <a href="LaunchingPCC.html">Launching PCC</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="UninstallingPCC.html">Uninstalling PCC</a>
                  </li>
                  <li>
                     <a href="UpgradingPCC.html">Upgrading PCC</a>
                  </li>
                  <li>
                     <a href="UsingPCC.html">Using PCC</a>
                     <ul>
                        <li>
                           <a href="PCCUIOverview.html">PCC UI Overview</a>
                        </li>
                        <li>
                           <a href="LoggingIn.html">Logging In</a>
                        </li>
                        <li>
                           <a href="PCCUISettings.html">PCC UI Settings</a>
                        </li>
                        <li>
                           <a href="PCCUIUserManagement.html">PCC UI User Management</a>
                        </li>
                        <li>
                           <a href="ClusterStatusPage.html">Cluster Status Page</a>
                        </li>
                        <li>
                           <a href="ConfiguringandDeployingaCluster.html">Configuring and Deploying a Cluster</a>
                           <ul>
                              <li>
                                 <a href="LaunchingtheAddClusterWizard.html">Launching the Add Cluster Wizard</a>
                              </li>
                              <li>
                                 <a href="CreateClusterDefinition.html">Create Cluster Definition</a>
                              </li>
                              <li>
                                 <a href="VersionsServicesandHosts.html">Versions, Services, and Hosts</a>
                              </li>
                              <li>
                                 <a href="HostVerification.html">Host Verification</a>
                              </li>
                              <li>
                                 <a href="ClusterTopology.html">Cluster Topology</a>
                              </li>
                              <li>
                                 <a href="ClusterConfiguration.html">Cluster Configuration</a>
                              </li>
                              <li>
                                 <a href="Validation.html">Validation</a>
                              </li>
                              <li>
                                 <a href="DeploymentStatus.html">Deployment Status</a>
                              </li>
                              <li>
                                 <a href="Summary.html">Summary</a>
                              </li>
                              <li>
                                 <a href="StartingtheClusterfromtheUI.html">Starting the Cluster from the UI</a>
                              </li>
                              <li>
                                 <a href="InitializingandConfiguringHAWQ.html">Initializing and Configuring HAWQ</a>
                              </li>
                           </ul>
                        </li>
                        <li>
                           <a href="StartingStoppingandUninstallingaCluster.html">Starting, Stopping, and Uninstalling a Cluster</a>
                        </li>
                        <li>
                           <a href="PCCUIDashboard.html">PCC UI Dashboard</a>
                        </li>
                        <li>
                           <a href="ClusterAnalysis.html">Cluster Analysis</a>
                        </li>
                        <li>
                           <a href="MapReduceJobMonitor.html">MapReduce Job Monitor</a>
                        </li>
                        <li>
                           <a href="YarnAppMonitor.html">Yarn App Monitor</a>
                        </li>
                        <li>
                           <a href="HAWQQueryMonitor.html">HAWQ Query Monitor</a>
                        </li>
                        <li>
                           <a href="Topology.html">Topology</a>
                        </li>
                        <li>
                           <a href="Logs.html">Logs</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="ConfiguringPCCforLDAP.html">Configuring PCC for LDAP</a>
                  </li>
                  <li>
                     <a href="CommandLineReference.html">Command Line Reference</a>
                  </li>
               </ul>
            </li>
         </ul>

         <p class="productname"><a href="PivotalHAWQ.html">Pivotal HAWQ</a></p>
         <ul>
            <li>
               <a href="HAWQ1.2.1ReleaseNotes.html">HAWQ 1.2.1 Release Notes</a>
            </li>
            <li>
               <a href="HAWQInstallationandUpgrade.html">HAWQ Installation and Upgrade</a>
               <ul>
                  <li>
                     <a href="PreparingtoInstallHAWQ.html">Preparing to Install HAWQ</a>
                  </li>
                  <li>
                     <a href="InstallingHAWQ.html">Installing HAWQ</a>
                  </li>
                  <li>
                     <a href="InstallingtheHAWQComponents.html">Installing the HAWQ Components</a>
                  </li>
                  <li>
                     <a href="UpgradingHAWQandComponents.html">Upgrading HAWQ and Components</a>
                  </li>
                  <li>
                     <a href="HAWQConfigurationParameterReference.html">HAWQ Configuration Parameter Reference</a>
                  </li>
               </ul>
            </li>
            <li>
               <a href="HAWQAdministration.html">HAWQ Administration</a>
               <ul>
                  <li>
                     <a href="HAWQOverview.html">HAWQ Overview</a>
                  </li>
                  <li>
                     <a href="HAWQQueryProcessing.html">HAWQ Query Processing</a>
                  </li>
                  <li>
                     <a href="UsingHAWQtoQueryData.html">Using HAWQ to Query Data</a>
                  </li>
                  <li>
                     <a href="UsingProceduralLanguages.html">Using Procedural Languages</a>
                  </li>
                  <li>
                     <a href="ConfiguringClientAuthentication.html">Configuring Client Authentication</a>
                  </li>
                  <li>
                     <a href="ConfiguringKerberosAuthentication.html">Configuring Kerberos Authentication</a>
                  </li>
                  <li>
                     <a href="ConfiguringLDAPAuthentication.html">Configuring LDAP Authentication</a>
                  </li>
                  <li>
                     <a href="BackingUpandRestoringHAWQDatabases.html">Backing Up and Restoring HAWQ Databases</a>
                  </li>
                  <li>
                     <a href="ExpandingtheHAWQSystem.html">Expanding the HAWQ System</a>
                  </li>
                  <li>
                     <a href="HAWQInputFormatforMapReduce.html">HAWQ InputFormat for MapReduce</a>
                  </li>
                  <li>
                     <a href="HAWQFilespacesandHighAvailabilityEnabledHDFS.html">HAWQ Filespaces and High Availability Enabled HDFS</a>
                  </li>
                  <li>
                     <a href="SQLCommandReference.html">SQL Command Reference</a>
                     <ul>
                        <li>
                           <a href="ABORT.html">ABORT</a>
                        </li>
                        <li>
                           <a href="ANALYZE.html">ANALYZE</a>
                        </li>
                        <li>
                           <a href="ALTERAGGREGATE.html">ALTER AGGREGATE</a>
                        </li>
                        <li>
                           <a href="ALTERFUNCTION.html">ALTER FUNCTION</a>
                        </li>
                        <li>
                           <a href="ALTEROPERATOR.html">ALTER OPERATOR</a>
                        </li>
                        <li>
                           <a href="ALTEROPERATORCLASS.html">ALTER OPERATOR CLASS</a>
                        </li>
                        <li>
                           <a href="ALTERROLE.html">ALTER ROLE</a>
                        </li>
                        <li>
                           <a href="ALTERTABLE.html">ALTER TABLE</a>
                        </li>
                        <li>
                           <a href="ALTERTABLESPACE.html">ALTER TABLESPACE</a>
                        </li>
                        <li>
                           <a href="ALTERTYPE.html">ALTER TYPE</a>
                        </li>
                        <li>
                           <a href="ALTERUSER.html">ALTER USER</a>
                        </li>
                        <li>
                           <a href="BEGIN.html">BEGIN</a>
                        </li>
                        <li>
                           <a href="CHECKPOINT.html">CHECKPOINT</a>
                        </li>
                        <li>
                           <a href="CLOSE.html">CLOSE</a>
                        </li>
                        <li>
                           <a href="COMMIT.html">COMMIT</a>
                        </li>
                        <li>
                           <a href="COPY.html">COPY</a>
                        </li>
                        <li>
                           <a href="CREATEAGGREGATE.html">CREATE AGGREGATE</a>
                        </li>
                        <li>
                           <a href="CREATEDATABASE.html">CREATE DATABASE</a>
                        </li>
                        <li>
                           <a href="CREATEEXTERNALTABLE.html">CREATE EXTERNAL TABLE</a>
                        </li>
                        <li>
                           <a href="CREATEFUNCTION.html">CREATE FUNCTION</a>
                        </li>
                        <li>
                           <a href="CREATEGROUP.html">CREATE GROUP</a>
                        </li>
                        <li>
                           <a href="CREATELANGUAGE.html">CREATE LANGUAGE</a>
                        </li>
                        <li>
                           <a href="CREATEOPERATOR.html">CREATE OPERATOR</a>
                        </li>
                        <li>
                           <a href="CREATEOPERATORCLASS.html">CREATE OPERATOR CLASS</a>
                        </li>
                        <li>
                           <a href="CREATERESOURCEQUEUE.html">CREATE RESOURCE QUEUE</a>
                        </li>
                        <li>
                           <a href="CREATEROLE.html">CREATE ROLE</a>
                        </li>
                        <li>
                           <a href="CREATESCHEMA.html">CREATE SCHEMA</a>
                        </li>
                        <li>
                           <a href="CREATESEQUENCE.html">CREATE SEQUENCE</a>
                        </li>
                        <li>
                           <a href="CREATETABLE.html">CREATE TABLE</a>
                        </li>
                        <li>
                           <a href="CREATETABLEAS.html">CREATE TABLE AS</a>
                        </li>
                        <li>
                           <a href="CREATETABLESPACE.html">CREATE TABLESPACE</a>
                        </li>
                        <li>
                           <a href="CREATETYPE.html">CREATE TYPE</a>
                        </li>
                        <li>
                           <a href="CREATE-USER.html">CREATE USER</a>
                        </li>
                        <li>
                           <a href="CREATEVIEW.html">CREATE VIEW</a>
                        </li>
                        <li>
                           <a href="DEALLOCATE.html">DEALLOCATE</a>
                        </li>
                        <li>
                           <a href="DECLARE.html">DECLARE</a>
                        </li>
                        <li>
                           <a href="DROPAGGREGATE.html">DROP AGGREGATE</a>
                        </li>
                        <li>
                           <a href="DROPDATABASE.html">DROP DATABASE</a>
                        </li>
                        <li>
                           <a href="DROPEXTERNALTABLE.html">DROP EXTERNAL TABLE</a>
                        </li>
                        <li>
                           <a href="DROPFILESPACE.html">DROP FILESPACE</a>
                        </li>
                        <li>
                           <a href="DROPFUNCTION.html">DROP FUNCTION</a>
                        </li>
                        <li>
                           <a href="DROPGROUP.html">DROP GROUP</a>
                        </li>
                        <li>
                           <a href="DROPOPERATOR.html">DROP OPERATOR</a>
                        </li>
                        <li>
                           <a href="DROPOPERATORCLASS.html">DROP OPERATOR CLASS</a>
                        </li>
                        <li>
                           <a href="DROPOWNED.html">DROP OWNED</a>
                        </li>
                        <li>
                           <a href="DROPRESOURCEQUEUE.html">DROP RESOURCE QUEUE</a>
                        </li>
                        <li>
                           <a href="DROPROLE.html">DROP ROLE</a>
                        </li>
                        <li>
                           <a href="DROPSCHEMA.html">DROP SCHEMA</a>
                        </li>
                        <li>
                           <a href="DROPSEQUENCE.html">DROP SEQUENCE</a>
                        </li>
                        <li>
                           <a href="DROPTABLE.html">DROP TABLE</a>
                        </li>
                        <li>
                           <a href="DROPTABLESPACE.html">DROP TABLESPACE</a>
                        </li>
                        <li>
                           <a href="DROPTYPE.html">DROP TYPE</a>
                        </li>
                        <li>
                           <a href="DROP-USER.html">DROP USER</a>
                        </li>
                        <li>
                           <a href="DROPVIEW.html">DROP VIEW</a>
                        </li>
                        <li>
                           <a href="END.html">END</a>
                        </li>
                        <li>
                           <a href="EXECUTE.html">EXECUTE</a>
                        </li>
                        <li>
                           <a href="EXPLAIN.html">EXPLAIN</a>
                        </li>
                        <li>
                           <a href="FETCH.html">FETCH</a>
                        </li>
                        <li>
                           <a href="GRANT.html">GRANT</a>
                        </li>
                        <li>
                           <a href="INSERT.html">INSERT</a>
                        </li>
                        <li>
                           <a href="PREPARE.html">PREPARE</a>
                        </li>
                        <li>
                           <a href="REASSIGNOWNED.html">REASSIGN OWNED</a>
                        </li>
                        <li>
                           <a href="RELEASESAVEPOINT.html">RELEASE SAVEPOINT</a>
                        </li>
                        <li>
                           <a href="RESET.html">RESET</a>
                        </li>
                        <li>
                           <a href="REVOKE.html">REVOKE</a>
                        </li>
                        <li>
                           <a href="ROLLBACK.html">ROLLBACK</a>
                        </li>
                        <li>
                           <a href="ROLLBACKTOSAVEPOINT.html">ROLLBACK TO SAVEPOINT</a>
                        </li>
                        <li>
                           <a href="SAVEPOINT.html">SAVEPOINT</a>
                        </li>
                        <li>
                           <a href="SELECT.html">SELECT</a>
                        </li>
                        <li>
                           <a href="SELECTINTO.html">SELECT INTO</a>
                        </li>
                        <li>
                           <a href="SET.html">SET</a>
                        </li>
                        <li>
                           <a href="SETROLE.html">SET ROLE</a>
                        </li>
                        <li>
                           <a href="SETSESSIONAUTHORIZATION.html">SET SESSION AUTHORIZATION</a>
                        </li>
                        <li>
                           <a href="SHOW.html">SHOW</a>
                        </li>
                        <li>
                           <a href="TRUNCATE.html">TRUNCATE</a>
                        </li>
                        <li>
                           <a href="VACUUM.html">VACUUM</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="ManagementUtilityReference.html">Management Utility Reference</a>
                     <ul>
                        <li>
                           <a href="gpactivatestandby.html">gpactivatestandby</a>
                        </li>
                        <li>
                           <a href="gpcheckperf.html">gpcheckperf</a>
                        </li>
                        <li>
                           <a href="gpconfig.html">gpconfig</a>
                        </li>
                        <li>
                           <a href="gpexpand.html">gpexpand</a>
                        </li>
                        <li>
                           <a href="gpextract.html">gpextract</a>
                        </li>
                        <li>
                           <a href="gpfdist.html">gpfdist</a>
                        </li>
                        <li>
                           <a href="gpfilespace.html">gpfilespace</a>
                        </li>
                        <li>
                           <a href="gpinitstandby.html">gpinitstandby</a>
                        </li>
                        <li>
                           <a href="gpinitsystem.html">gpinitsystem</a>
                        </li>
                        <li>
                           <a href="gpload.html">gpload</a>
                        </li>
                        <li>
                           <a href="gplogfilter.html">gplogfilter</a>
                        </li>
                        <li>
                           <a href="gprecoverseg.html">gprecoverseg</a>
                        </li>
                        <li>
                           <a href="gpscp.html">gpscp</a>
                        </li>
                        <li>
                           <a href="gpstart.html">gpstart</a>
                        </li>
                        <li>
                           <a href="gpssh.html">gpssh</a>
                        </li>
                        <li>
                           <a href="gssh-exkeys.html">gssh-exkeys</a>
                        </li>
                        <li>
                           <a href="gpstate.html">gpstate</a>
                        </li>
                        <li>
                           <a href="gpstop.html">gpstop</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="ClientUtilityReference.html">Client Utility Reference</a>
                     <ul>
                        <li>
                           <a href="createdb.html">createdb</a>
                        </li>
                        <li>
                           <a href="createlang.html">createlang</a>
                        </li>
                        <li>
                           <a href="createuser.html">createuser</a>
                        </li>
                        <li>
                           <a href="dropdb.html">dropdb</a>
                        </li>
                        <li>
                           <a href="dropuser.html">dropuser</a>
                        </li>
                        <li>
                           <a href="pg_dump.html">pg_dump</a>
                        </li>
                        <li>
                           <a href="pg_dumpall.html">pg_dumpall</a>
                        </li>
                        <li>
                           <a href="pg_restore.html">pg_restore</a>
                        </li>
                        <li>
                           <a href="psql.html">psql</a>
                        </li>
                        <li>
                           <a href="vacuumdb.html">vacuumdb</a>
                        </li>
                     </ul>
                  </li>
                  <li>
                     <a href="HAWQServerConfigurationParameters.html">HAWQ Server Configuration Parameters</a>
                  </li>
                  <li>
                     <a href="CharacterSetSupportReference.html">Character Set Support Reference</a>
                  </li>
                  <li>
                     <a href="HAWQEnvironmentVariables.html">HAWQ Environment Variables</a>
                  </li>
                  <li>
                     <a href="HAWQDataTypes.html">HAWQ Data Types</a>
                  </li>
                  <li>
                     <a href="SystemCatalogReference.html">System Catalog Reference</a>
                  </li>
                  <li>
                     <a href="hawq_toolkitReference.html">hawq_toolkit Reference</a>
                  </li>
                  <li>
                     <a href="ManagingHAWQLogFiles.html">Managing HAWQ Log Files</a>
                  </li>
               </ul>
            </li>
            <li>
               <a href="PivotalExtensionFrameworkPXF.html">Pivotal Extension Framework (PXF)</a>
               <ul>
                  <li>
                     <a class="active" href="PXFInstallationandAdministration.html">PXF Installation and Administration</a>
                  </li>
                  <li>
                     <a href="PXFExternalTableandAPIReference.html">PXF External Table and API Reference</a>
                  </li>
               </ul>
            </li>
         </ul>

</div>
   
	
	
	<main class="content content-layout" id="js-content" role="main">
        <a id="top"></a>
        
          <h1 class="title-container">PXF Installation and Administration</h1>
          <div id="js-quick-links"></div>          
        
        <div class="to-top" id="js-to-top">
          <a href="#top" title="back to top"></a>
        </div>     
        
		
		<div style="visibility:hidden; height:2px;">Pivotal Product Documentation : PXF Installation and Administration</div><div class="wiki-content group" id="main-content">
<p align="LEFT">PXF is an extensible framework that allows HAWQ to query external system data.&nbsp;PXF includes built-in connectors for accessing data that exists inside HDFS files, Hive tables, and HBase tables. Users can also create their own connectors to other parallel data stores or processing engines.&nbsp;To create these connectors using <span style="line-height: 1.4285;">JAVA plugins, see the <a href="PXFExternalTableandAPIReference.html"><span><em>PXF</em></span><em>&nbsp;External Table and API Reference</em></a></span><span style="line-height: 1.4285;">.</span></p><h2 id="PXFInstallationandAdministration-Prerequisites"><span style="line-height: 1.4285;">Prerequisites</span></h2><p>Check that the following systems are installed and running before you install PXF:</p><ul><li>HAWQ</li><li>Pivotal Hadoop (PHD)</li><li>Hadoop File System (HDFS)</li><li><span style="line-height: 1.4285715;">When configuring Secure (Kerberized) HDFS,&nbsp;for PXF to function, the NameNode port must be 8020. This limitation will be removed in the next release.</span></li></ul><h2 id="PXFInstallationandAdministration-InstallingPXF"><span style="line-height: 1.5;">Installing PXF</span></h2><p>PXF is installed through the Pivotal Installation and Configuration manager (ICM). Please refer to ICM documentation for exact commands to install, start and stop PXF.</p><h2 id="PXFInstallationandAdministration-ConfiguringPXF">Configuring PXF</h2><h3 id="PXFInstallationandAdministration-SettinguptheJavaClasspath">Setting up the Java Classpath</h3><p align="LEFT">The classpath of PXF service is set by ICM during installation. Administrators should only modify it when adding new PXF connectors. The classpath is defined in two files:</p><ol><li><em>/etc/gphd/pxf/conf/pxf-private.classpath</em>&nbsp;- contains all the required resources to run the service, including pxf-hdfs, pxf-hbase and pxf-hive plugins.<br>This file must not be edited or removed.</li><li><em>/etc/gphd/pxf/conf/pxf-public.classpath</em> - user defined resources can be added here, for example for running a user defined plugin.<br>The classpath resources should be defined one per line. Wildcard characters can be used in the name of the resource, but not in the full path.</li></ol><p>After changing the classpath files, the PXF service must be restarted.&nbsp;<br>Resources can also be added to the&nbsp;staging dir&nbsp;<em>/usr/lib/gphd/publicstage</em>&nbsp;(see<em>&nbsp;<a class="active" href="PXFInstallationandAdministration.html">About the Public Directory</a>&nbsp;</em>below).</p><h3 id="PXFInstallationandAdministration-SettinguptheJVMCommandLineOptionsforPXFService">Setting up the JVM Command Line Options for PXF Service</h3><p align="LEFT">The pxf service JVM command line options can be added/modified for each pxf-service instance in the following file:</p><ul><li>/var/gphd/pxf/pxf-service/bin/setenv.sh</li></ul><p align="LEFT">Currently the JVM_OPT are set with following values for <span style="color: rgb(0,0,0);">maximum Java heap size&nbsp;</span>and <span style="color: rgb(0,0,0);">thread stack size</span>&nbsp;:</p><ul><li>JVM_OPTS="-Xmx512M -Xss256K"</li></ul><p>After adding/modifying&nbsp;the JVM command line options, the PXF service must be restarted.</p><h3 id="PXFInstallationandAdministration-SecurePXF"><span style="color: rgb(0,0,0);font-size: 20.0px;font-weight: normal;line-height: 1.5;">Secure PXF</span></h3><p><span style="color: rgb(0,0,0);">PXF can be used on a secure HDFS cluster.&nbsp;</span><span style="color: rgb(0,0,0);line-height: 1.4285;background-color: transparent;">Read, write and analyze operations for PXF tables on HDFS files are enabled.&nbsp;</span><span style="color: rgb(0,0,0);line-height: 1.4285;background-color: transparent;">It requires no changes to prexisting PXF tables from a previous version.</span></p><h3 id="PXFInstallationandAdministration-Requirements"><span style="color: rgb(0,0,0);line-height: 1.5625;font-size: 16.0px;">Requirements</span></h3><ul style="list-style-type: square;"><li><span style="color: rgb(0,0,0);">Both HDFS and YARN principals are created and are properly configured.</span></li><li><span style="color: rgb(0,0,0);">HDFS uses port 8020 (see&nbsp;<a class="active" href="PXFInstallationandAdministration.html">Limitations</a> below).</span></li><li><p><span style="color: rgb(0,0,0);">HAWQ is correctly configured to work in secure mode, according to the instructions in the HAWQ guide.</span></p> <div class="aui-message warning shadowed information-macro">
<p class="title">Note</p>
<span class="aui-icon icon-warning">Icon</span>
<div class="message-content">
<p>-The HDFS Namenode port must be 8020. This is a limitation that will be fixed in the next PXF version.</p><p><span style="color: rgb(0,0,0);">-Please refer to the troubleshooting section for common errors related to PXF security, and their meaning.</span></p>
</div>
</div>
</li></ul><h2 id="PXFInstallationandAdministration-ReadingandWritingDatawithPXF"><span style="color: rgb(0,0,0);line-height: 1.25;font-size: 24.0px;">Reading and Writing&nbsp;Data with PXF</span></h2><p>PXF comes with a number of built-in connectors for reading data that exists inside HDFS files, Hive tables, HBase tables, and for writing data into HDFS files. These built-in&nbsp;connectors use the PXF extensible API. You can also use the extensible API to create&nbsp;your own connectors to any other type of parallel data store or processing engine. See&nbsp;<a href="PXFExternalTableandAPIReference.html">PXF External Table and API Reference</a>&nbsp;for more information about the API.</p><p>This&nbsp;topic contains the following information:</p><ul><li>Accessing HDFS File Data with PXF (Read + Write)</li><li>Accessing HIVE Data with PXF (Read only)</li><li>Accessing HBase Data with PXF (Read only)</li><li>Accessing GemFireXD Data with PXF (Read only)</li></ul><h3 id="PXFInstallationandAdministration-Built-inProfiles">Built-in Profiles</h3><p><span>A profile is a collection of common metadata attributes. Use the convenient and simplified PXF syntax.</span>&nbsp;</p><p>PXF comes with a number of built-in profiles that group together&nbsp;a collection of metadata attributes to achieve a common goal:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Profile</th><th class="confluenceTh">Description</th><th class="confluenceTh" colspan="1">Fragmenter/Accessor/Resolver</th></tr><tr><td class="confluenceTd">HdfsTextSimple</td><td class="confluenceTd"><p>Read or write delimited single line records from or to plain text files on HDFS.</p></td><td class="confluenceTd" colspan="1"><ul><li><span>com.pivotal.pxf.plugins.hdfs.</span>HdfsDataFragmenter</li><li><span>com.pivotal.pxf.plugins.hdfs.</span>LineBreakAccessor</li><li><span>com.pivotal.pxf.plugins.hdfs.</span>StringPassResolver</li></ul></td></tr><tr><td class="confluenceTd">HdfsTextMulti</td><td class="confluenceTd"><p>Read delimited single or multi-line records (with quoted linefeeds) from plain text files on <br>HDFS.<br>This profile is not splittable (non parallel); therefore reading is slower than reading with <br>HdfsTextSimple.</p></td><td class="confluenceTd" colspan="1"><ul><li><span>com.pivotal.pxf.plugins.hdfs.</span>HdfsDataFragmenter</li><li><span>com.pivotal.pxf.plugins.hdfs.</span>QuotedLineBreakAccessor</li><li><span>com.pivotal.pxf.plugins.hdfs.</span>StringPassResolver</li></ul></td></tr><tr><td class="confluenceTd">Hive</td><td class="confluenceTd">Use this&nbsp;when connecting to Hive. The Hive table may consist of any storage types.</td><td class="confluenceTd" colspan="1"><ul><li><span>com.pivotal.pxf.plugins.hive.</span>HiveDataFragmenter</li><li><span>com.pivotal.pxf.plugins.hive.</span>HiveAccessor</li><li><span>com.pivotal.pxf.plugins.hive.</span>HiveResolver</li></ul></td></tr><tr><td class="confluenceTd" colspan="1">HiveRC</td><td class="confluenceTd" colspan="1"><p>Use this when connecting to a Hive table where each partition is stored as RCFile. It is optimized for it</p><p>&nbsp;</p></td><td class="confluenceTd" colspan="1"><ul><li>com.pivotal.pxf.plugins.hive.HiveInputFormatFragmenter</li><li>com.pivotal.pxf.plugins.hive.HiveRCFileAccessor</li><li>com.pivotal.pxf.plugins.hive.HiveColumnarSerdeResolver</li></ul></td></tr><tr><td class="confluenceTd">HBase</td><td class="confluenceTd">Use this when&nbsp;connected to an HBase data store engine.</td><td class="confluenceTd" colspan="1"><ul><li><span>com.pivotal.pxf.plugins.hbase.</span>HBaseDataFragmenter</li><li><span>com.pivotal.pxf.plugins.hbase.</span>HBaseAccessor</li><li><span>com.pivotal.pxf.plugins.hbase.</span>HBaseResolver</li></ul><p>&nbsp;</p></td></tr><tr><td class="confluenceTd">Avro</td><td class="confluenceTd">Reading Avro files (i.e fileName.avro).</td><td class="confluenceTd" colspan="1"><ul><li><span>com.pivotal.pxf.plugins.hdfs.</span>HdfsDataFragmenter</li><li><span>com.pivotal.pxf.plugins.hdfs.</span>AvroFileAccessor</li><li><span>com.pivotal.pxf.plugins.hdfs.</span>AvroResolver</li></ul></td></tr><tr><td class="confluenceTd">GemFireXD</td><td class="confluenceTd">Use this when&nbsp;connected to GemFireXD</td><td class="confluenceTd" colspan="1"><ul><li>com.pivotal.pxf.plugins.gemfirexd.GemFireXDFragmenter</li><li>com.pivotal.pxf.plugins.gemfirexd.GemFireXDAccessor</li></ul></td></tr></tbody></table></div><h4 id="PXFInstallationandAdministration-AddingandUpdatingProfiles">Adding and Updating Profiles</h4><p class="diff-block-target diff-block-context"><span style="color: rgb(51,51,51);">Administrators can add new profiles or edit the built-in profiles inside&nbsp;</span><span style="color: rgb(51,51,51);"><em>pxf-profiles.xm</em>l (and apply them with the Pivotal Hadoop (HD) Enterprise Command Line Interface). You can use the all the profiles in <em>pxf-profiles.xml</em>.</span></p><p class="diff-block-target diff-block-context">Each profile has a mandatory unique&nbsp;name and an optional&nbsp;description.</p><p class="diff-block-target diff-block-context">In addition, each profile contains a set of plugins&nbsp;that&nbsp;are an&nbsp;extensible set of metadata attributes.</p><h4 class="diff-block-target diff-block-context" id="PXFInstallationandAdministration-CustomProfileExample">Custom Profile Example</h4><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;profile&gt; 
 &lt;name&gt;MyCustomProfile&lt;/name&gt;
 &lt;description&gt;A Custom Profile Example&lt;/description&gt;
 &lt;plugins&gt;
 	&lt;fragmenter&gt;package.name.CustomProfileFragmenter&lt;/fragmenter&gt;
 	&lt;accessor&gt;package.name.CustomProfileAccessor&lt;/accessor&gt;
 	&lt;customPlugin1&gt;package.name.MyCustomPluginValue1&lt;/customPlugin1&gt;
 	&lt;customPlugin2&gt;package.name.MyCustomPluginValue2&lt;/customPlugin2&gt;
 &lt;/plugins&gt;
&lt;/profile&gt;</pre>
</div></div><h4 id="PXFInstallationandAdministration-DeprecatedClassnames">Deprecated Classnames</h4><p>In past versions of PXF, connector class names could be used without their package names:<br>e.g. HdfsDataFragmenter instead of com.pivotal.pxf.plugins.hdfs.HdfsDataFragmenter.</p><p>This has changed in the last version.<br>While package-less classes can still be used, a warning will be issued upon creation and use of any table.</p><div class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent panelContent">
<pre>WARNING:  Use of HdfsDataFragmenter is deprecated and it will be removed on the next major version
DETAIL:  Please use the appropriate PXF profile for forward compatibility (e.g. profile=HdfsTextSimple)</pre>
</div></div><p>Please note that the next major release will not support the old names.<br>That means a "class not found" error message will be issued.</p><p>To avoid future deprecation issues, PXF Profiles should be used.<br><span style="line-height: 1.4285;">Recommended built-in PXF profiles&nbsp;</span></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Old name</th><th class="confluenceTh">Profile</th></tr><tr><td class="confluenceTd" colspan="1">HdfsDataFragmenter, TextFileAccessor, TextResolver</td><td class="confluenceTd" colspan="1">HdfsTextSimple</td></tr><tr><td class="confluenceTd" colspan="1">HdfsDataFragmenter, QuotedLineBreakAccessor, TextResolver</td><td class="confluenceTd" colspan="1">HdfsTextMulti</td></tr><tr><td class="confluenceTd" colspan="1">HdfsDataFragmenter, AvroFileAccessor, AvroResolver</td><td class="confluenceTd" colspan="1">Avro</td></tr><tr><td class="confluenceTd" colspan="1">HdfsDataFragmenter, SequenceFileAccessor, CustomWritable</td><td class="confluenceTd" colspan="1">SequenceWritable</td></tr><tr><td class="confluenceTd" colspan="1">HBaseDataFragmenter, HBaseAccessor, HBaseResolver</td><td class="confluenceTd" colspan="1">HBase</td></tr><tr><td class="confluenceTd" colspan="1">HiveDataFragmenter, HiveAccessor, HiveResolver</td><td class="confluenceTd" colspan="1">Hive</td></tr></tbody></table></div><p>&nbsp;</p><p>The following table shows old versus new class names .</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Old Name</th><th class="confluenceTh">New Name</th></tr><tr><td class="confluenceTd" colspan="1">TextFileAccessor, LineBreakAccessor, LineReaderAccessor</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.LineBreakAccessor</td></tr><tr><td class="confluenceTd" colspan="1">QuotedLineBreakAccessor</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.QuotedLineBreakAccessor</td></tr><tr><td class="confluenceTd" colspan="1">AvroFileAccessor</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.AvroFileAccessor</td></tr><tr><td class="confluenceTd" colspan="1">SequenceFileAccessor</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.SequenceFileAccessor</td></tr><tr><td class="confluenceTd" colspan="1">TextResolver, StringPassResolver</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.StringPassResolver</td></tr><tr><td class="confluenceTd" colspan="1">AvroResolver</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.AvroResolver</td></tr><tr><td class="confluenceTd" colspan="1">WritableResolver</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.WritableResolver</td></tr><tr><td class="confluenceTd" colspan="1">HdfsDataFragmenter</td><td class="confluenceTd" colspan="1">com.pivotal.pxf.plugins.hdfs.HdfsDataFragmenter</td></tr></tbody></table></div><h2 class="diff-block-target diff-block-context" id="PXFInstallationandAdministration-AccessingHDFSFileDatawithPXF">Accessing HDFS File Data with PXF&nbsp;</h2><h3 id="PXFInstallationandAdministration-InstallingthePXFHDFSplugin">Installing the PXF HDFS plugin</h3><p>Install the PXF HDFS plugin jar file on all nodes in the cluster:</p><p>&nbsp;</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: bash; gutter: false" style="font-size:12px;">sudo rpm -i pxf-hdfs-2.3.0.0-x.rpm</pre>
</div></div><ul><li class="Code">PXF RPMs reside in the Pivotal ADS/HAWQ stack file.&nbsp;</li><li>The script installs the JAR file at the default location at&nbsp;<em>/usr/lib/gphd/pxf-2.3.0.0</em>. The S<em>oftlink pxf-hdfs.jar</em>&nbsp;will be created in&nbsp;<em>/usr/lib/gphd/pxf</em></li></ul><p>&nbsp;</p> <div class="aui-message hint shadowed information-macro">
<p class="title">Notes</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<ul><li>Pivotal recommends that you test PXF on HDFS before connecting to Hive or HBase.</li><li>PXF on secure HDFS clusters requires NameNode to be configured on port 8020.</li><li>HBase/Hive configurations requiring user authentication are not supported.</li></ul>
</div>
</div>
<p>The syntax for accessing an HDFS file is as follows:&nbsp;</p><h3 id="PXFInstallationandAdministration-Syntax">Syntax</h3><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE [READABLE|WRITABLE] EXTERNAL TABLE &lt;tbl name&gt; (&lt;attr list&gt;)
LOCATION ('pxf://&lt;name node hostname:50070&gt;/&lt;path to file or directory&gt;?Profile=&lt;chosen profile&gt;[&amp;&lt;additional options&gt;=&lt;value&gt;]')
FORMAT '[TEXT | CSV | CUSTOM]' (&lt;formatting properties&gt;)
[ [LOG ERRORS INTO &lt;error_table&gt;] SEGMENT REJECT LIMIT &lt;count&gt; [ROWS | PERCENT] ];

SELECT ... FROM &lt;tbl name&gt;; --to read from hdfs with READABLE table.
INSERT INTO &lt;tbl name&gt; ...; --to write to hdfs with WRITABLE table.</pre>
</div></div><p>To read the data in the files or to write based on the&nbsp;existing format, you need to select the FORMAT, Profile, or one of the classes.</p><p>This topic describes the following:</p><ul><li>FORMAT clause</li><li>Fragmenter</li><li>Accessor</li><li>Resolver</li></ul> <div class="aui-message hint shadowed information-macro">
<p class="title">Note</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p>For more details about the API and classes, see the <a href="PXFExternalTableandAPIReference.html"><em>Pivotal Extension Framework API and Reference Guide</em></a>.</p>
</div>
</div>
<h3 id="PXFInstallationandAdministration-FORMATclause"><span style="line-height: 1.4285;">FORMAT clause</span></h3><p><span style="line-height: 1.4285;">To read data, use the following formats with any PXF connector:</span></p><ul><li><strong>FORMAT ‘TEXT’</strong>: Use with plain delimited text files on HDFS.</li><li><strong>FORMAT ‘CSV’</strong>: Use with comma-separated value files on HDFS.</li><li><strong>FORMAT ‘CUSTOM’</strong>: Use with other files, such as binary formats. Must always be used with built-in formatter ‘<em>pxfwritable_import</em>’ (for read) or '<em>pxfwritable_export</em>' (for write).</li></ul><h3 id="PXFInstallationandAdministration-Fragmenter">Fragmenter</h3><p>Always use either&nbsp;[<em>HdfsTextSimple |&nbsp;HdfsTextMulti]</em> Profile or an&nbsp;<span>com.pivotal.pxf.plugins.hdfs.</span><em>HdfsDataFragmenter </em>for HDFS file data<em>.&nbsp;</em></p> <div class="aui-message hint shadowed information-macro">
<p class="title">Note</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p>For read tables, you must include a Profile or a Fragmenter in the table definition.</p>
</div>
</div>
<h3 id="PXFInstallationandAdministration-Accessor">Accessor</h3><p>The choice of an Accessor depends on the HDFS data file type.&nbsp;</p><p><strong>Note:</strong> You must include a Profile or an&nbsp;Accessor&nbsp;in the table definition.</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh"><div class="tablesorter-header-inner">File Type</div></th><th class="confluenceTh"><div class="tablesorter-header-inner">Accessor</div></th><th class="confluenceTh"><div class="tablesorter-header-inner">FORMAT clause</div></th><th class="confluenceTh" colspan="1"><div class="tablesorter-header-inner">Comments</div></th></tr><tr><td class="confluenceTd">Plain Text delimited</td><td class="confluenceTd"><span>com.pivotal.pxf.plugins.hdfs.</span>LineBreakAccessor</td><td class="confluenceTd">FORMAT 'TEXT' (&lt;format param list&gt;)</td><td class="confluenceTd" colspan="1">&nbsp;Read + Write</td></tr><tr><td class="confluenceTd" rowspan="2">Plain Text CSV&nbsp;</td><td class="confluenceTd"><p><span>com.pivotal.pxf.plugins.hdfs.</span>LineBreakAccessor</p></td><td class="confluenceTd" rowspan="2">FORMAT 'CSV' (&lt;format param list&gt;)&nbsp;</td><td class="confluenceTd" colspan="1"><p>LineBreakAccessor is parallel and faster.</p><p>Use if each logical data row is a physical data line.</p><p>Read + Write&nbsp;</p></td></tr><tr><td class="confluenceTd" colspan="1"><span>com.pivotal.pxf.plugins.hdfs.</span>QuotedLineBreakAccessor</td><td class="confluenceTd" colspan="1"><p>QuotedLineBreakAccessor is slower and non parallel.</p><p>Use if the data includes embedded (quoted) linefeed <br>characters.</p><p>Read Only&nbsp;</p></td></tr><tr><td class="confluenceTd">SequenceFile</td><td class="confluenceTd"><span>com.pivotal.pxf.plugins.hdfs.</span>SequenceFileAccessor</td><td class="confluenceTd">FORMAT 'CUSTOM' (formatter='pxfwritable_import')</td><td class="confluenceTd" colspan="1">&nbsp;Read + Write (use formatter='pxfwritable_export' for write)</td></tr><tr><td class="confluenceTd" colspan="1">AvroFile</td><td class="confluenceTd" colspan="1"><span>com.pivotal.pxf.plugins.hdfs.</span>AvroFileAccessor</td><td class="confluenceTd" colspan="1">FORMAT 'CUSTOM' (formatter='pxfwritable_import')</td><td class="confluenceTd" colspan="1">&nbsp;Read Only</td></tr></tbody></table></div><h3 id="PXFInstallationandAdministration-Resolver">Resolver</h3><p>Choose the Resolver format if data records are serialized in the HDFS file.&nbsp;</p><p><strong>Note</strong>: You must include a Profile or a Resolver in the table definition.</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh"><div class="tablesorter-header-inner">Record Serialization</div></th><th class="confluenceTh"><div class="tablesorter-header-inner">Resolver</div></th><th class="confluenceTh" colspan="1"><div class="tablesorter-header-inner">Comments</div></th></tr><tr><td class="confluenceTd">Avro</td><td class="confluenceTd"><span>com.pivotal.pxf.plugins.hdfs.</span>AvroResolver</td><td class="confluenceTd" colspan="1"><ul><li>Avro files include the record schema, Avro serialization can be used in other file types (e.g, Sequence File).&nbsp;</li><li>For Avro serialized records outside an Avro file, include a schema file name (.avsc)<span> in the url under <br>the optional&nbsp;</span><em>Schema-Data&nbsp;</em>option.</li><li><span>The schema file name must exist in the public stage directory.</span></li><li>Deserialize Only (Read)<span>&nbsp;.</span></li></ul></td></tr><tr><td class="confluenceTd">Java Writable</td><td class="confluenceTd"><span>com.pivotal.pxf.plugins.hdfs.</span>WritableResolver</td><td class="confluenceTd" colspan="1"><ul><li>Include the name of the Java class that uses Writable serialization&nbsp;<span>in the URL under the optional&nbsp;</span><em>Schema-Data.</em></li><li><span>The class file must exist in the public stage directory (or in Hadoop's class path).</span></li><li>Deserialize and Serialize (Read + Write).<span>&nbsp;</span></li><li><span><span>See </span></span><span style="line-height: 1.4285;"><a class="active" href="PXFInstallationandAdministration.html">Customized Writable Schema File Guidelines</a>.</span></li></ul></td></tr><tr><td class="confluenceTd">None (plain text)</td><td class="confluenceTd"><span>com.pivotal.pxf.plugins.hdfs.</span>StringPassResolver</td><td class="confluenceTd" colspan="1"><ul><li>Does not serialize plain text records. The database parses&nbsp;plain records. Passes records as they are.</li><li><span>Deserialize and Serialize (Read + Write).</span></li></ul></td></tr></tbody></table></div><h3 id="PXFInstallationandAdministration-AdditionalOptions">Additional Options</h3><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh"><div class="tablesorter-header-inner">Option Name</div></th><th class="confluenceTh"><div class="tablesorter-header-inner">Description</div></th></tr><tr><td class="confluenceTd"><span style="color: rgb(0,0,0);">COMPRESSION_CODEC</span></td><td class="confluenceTd"><ul><li>Useful for WRITABLE PXF tables.</li><li>Specifies the c<span style="color: rgb(0,0,0);">ompression codec class name for compressing the written data. The class must implement the&nbsp;org.apache.hadoop.io.compress.CompressionCodec <br></span><span style="color: rgb(0,0,0);">interface. </span></li><li><span style="color: rgb(0,0,0);">&nbsp;</span><span style="color: rgb(0,0,0);">Some valid values are</span><span style="color: rgb(0,0,0);">&nbsp;</span><span style="color: rgb(0,0,0);">org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec.</span></li><li><span style="color: rgb(0,0,0);"><span style="color: rgb(0,0,0);"> Note: org.apache.hadoop.io.compress.BZip2Codec runs in a single thread and can be slow.</span></span></li><li><span style="color: rgb(0,0,0);">This option has no default value.&nbsp;</span></li><li><span style="color: rgb(0,0,0);">When the option is not defined, no compression will be done.</span></li></ul></td></tr><tr><td class="confluenceTd" colspan="1">COMPRESSION_TYPE</td><td class="confluenceTd" colspan="1"><ul><li>Useful WRITABLE PXF tables with SequenceFileAccessor.</li><li>Ignored when COMPRESSION_CODEC is not defined.</li><li>Specifies the compression type for sequence file.</li><li>Valid options are:&nbsp;<ul><li>RECORD - only the value part of each row is compressed.</li><li>BLOCK - both keys and values are collected in 'blocks' separately and compressed.</li></ul></li><li>Default value: RECORD.</li></ul></td></tr><tr><td class="confluenceTd">SCHEMA-DATA</td><td class="confluenceTd"><p>The data schema file used to create and read&nbsp;the HDFS file. <span>For example, you could create an avsc (for Avro), or a Java class <br>(for Writable Serialization) file. Check that the file exists on the public </span><span>directory (see <em><a class="active" href="PXFInstallationandAdministration.html">About the Public Directory</a></em>).</span></p><p><span><span style="color: rgb(0,0,0);">This option has no default value.</span></span></p></td></tr><tr><td class="confluenceTd" colspan="1">THREAD-SAFE</td><td class="confluenceTd" colspan="1"><p>Determines if the table query can run in multithread mode or not. When set to FALSE, requests will be handled in a single thread.</p><p>Should be set when a plugin or other elements that are not thread safe are used (e.g. compression codec).</p><p>Allowed values: TRUE, FALSE. Default value is TRUE - requests can run in multithread mode.</p></td></tr><tr><td class="confluenceTd" colspan="1">&nbsp;&lt;custom&gt;</td><td class="confluenceTd" colspan="1">&nbsp;Any option added to the pxf URI string will be accepted and passed, along with its value, to the Fragmenter, Accessor, Analyzer and <span>Resolver implementations </span></td></tr></tbody></table></div><h3 id="PXFInstallationandAdministration-AccessingdataonaHighAvailabilityHDFScluster">Accessing data on a High Availability &nbsp;HDFS cluster</h3><p>To&nbsp;access data on a High Availability HDFS cluster, you need to change the authority&nbsp;in the URI in the LOCATION.<br>Use &lt;<strong>HA&nbsp;nameservice</strong>&gt; instead of &lt;<strong>name node host:50070</strong>&gt;.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE [READABLE|WRITABLE] EXTERNAL TABLE &lt;tbl name&gt; (&lt;attr list&gt;)
LOCATION ('pxf://&lt;HA nameservice&gt;/&lt;path to file or directory&gt;?Profile=&lt;chosen profile&gt;[&amp;&lt;additional options&gt;=&lt;value&gt;]')
FORMAT '[TEXT | CSV | CUSTOM]' (&lt;formatting properties&gt;);</pre>
</div></div><p>The opposite is true when a highly available HDFS cluster is reverted to a single namenode configuration. In that case, any table definition that has the nameservice specified should use the &lt;NN host&gt;:&lt;NN rest port&gt; syntax.&nbsp;</p><p>&nbsp;</p><p><span style="color: rgb(0,102,0);line-height: 1.5;font-size: 20.0px;">About the Public Directory</span></p><p align="LEFT">PXF provides a space to store your customized serializers and schema files on the filesystem. You must add schema files on all the datanodes and restart&nbsp;the cluster. The RPM creates the directory at the default location:&nbsp;<em>/usr/lib/gphd/publicstage</em>.</p><p align="LEFT">Ensure&nbsp;that all HDFS users have read permissions to HDFS services and limit&nbsp;write permissions&nbsp;to specific users.</p><h3 id="PXFInstallationandAdministration-Recordkeyinkey-valuefileformats">Record key in key-value &nbsp;file formats</h3><p>For sequence file and other file formats that store rows in a key-value format, the key value can be accessed through HAWQ by using the saved keyword '<em>recordkey</em>' as a field name.</p><p>The field type must correspond to the key type, much as the other fields must match the HDFS data.&nbsp;</p><p>WritableResolver supports read and write of recordkey, which can be of the following Writable Hadoop types:&nbsp;<br><span style="color: rgb(0,0,0);">BooleanWritable, ByteWritable, IntWritable, DoubleWritable, FloatWritable, LongWritable, Text.</span></p><p><span style="color: rgb(0,0,0);">If the <em>recordkey</em> field is not defined, the key is ignored in read, and a default value (segment id as LongWritable) is written in write.</span></p><h3 id="PXFInstallationandAdministration-Example"><span style="color: rgb(0,0,0);">Example</span></h3><p><span style="color: rgb(0,0,0);">Let's say we have a data schema Babies.class containing 3 fields: (name text, birthday text, weight float).&nbsp;</span></p><p><span style="color: rgb(0,0,0);">An external table must include these three fields, and can&nbsp;either include or&nbsp;ignore the recordkey.</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">-- writable table with recordkey
CREATE WRITABLE EXTERNAL TABLE babies_registry (recordkey int, name text, birthday text, weight float)
LOCATION ('pxf://namenode_host:50070/babies_1940s?
ACCESSOR=com.pivotal.pxf.plugins.hdfs.SequenceFileAccessor&amp;RESOLVER=com.pivotal.pxf.plugins.hdfs.WritableResolver&amp;DATA-SCHEMA=Babies')
FORMAT 'CUSTOM' (formatter='pxfwritable_export');
INSERT INTO babies_registry VALUES (123456, "James Paul McCartney", "June 18, 1942", 3.800);
-- writable table without recordkey
CREATE WRITABLE EXTERNAL TABLE babies_registry2 (name text, birthday text, weight float)
LOCATION ('pxf://namenode_host:50070/babies_1940s?ACCESSOR=com.pivotal.pxf.plugins.SequenceFileAccessor&amp;RESOLVER=com.pivotal.pxf.plugins.WritableResolver&amp;DATA-SCHEMA=Babies')
FORMAT 'CUSTOM' (formatter='pxfwritable_export');
INSERT INTO babies_registry VALUES ("Richard Starkey", "July 7, 1940", 4.0); -- this record's key will have some default value</pre>
</div></div><p>The same goes for reading data from an existing file with a key-value format, e.g. a Sequence file.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">-- readable table with recordkey
CREATE EXTERNAL TABLE babies_1940 (recordkey int, name text, birthday text, weight float)
LOCATION ('pxf://namenode_host:50070/babies_1940s?ACCESSOR=com.pivotal.pxf.plugins.hdfs.SequenceFileAccessor&amp;RESOLVER=com.pivotal.pxf.plugins.hdfs.WritableResolver&amp;DATA-SCHEMA=Babies')
FORMAT 'CUSTOM' (formatter='pxfwritable_import');
SELECT * FROM babies_1940; -- retrieves each record's key
-- readable table without recordkey
CREATE EXTERNAL TABLE babies_1940_2 (name text, birthday text, weight float)
LOCATION ('pxf://namenode_host:50070/babies_1940s?ACCESSOR=com.pivotal.pxf.plugins.hdfs.SequenceFileAccessor&amp;RESOLVER=com.pivotal.pxf.plugins.hdfs.WritableResolver&amp;DATA-SCHEMA=Babies')
FORMAT 'CUSTOM' (formatter='pxfwritable_import');
SELECT * FROM babies_1940_2; -- ignores the records' key</pre>
</div></div><h2 id="PXFInstallationandAdministration-CustomizedWritableSchemaFileGuidelines">Customized Writable Schema File Guidelines</h2><p><span style="color: rgb(0,0,0);">When using a WritableResolver, a schema file needs to be defined. The file needs to be a Java class file and must be on the class path of PXF.</span></p><p><span style="color: rgb(0,0,0);line-height: 1.4285;">The class file must follow the following requirements:</span></p><ol><li><span style="color: rgb(0,0,0);">Must implement org.apache.hadoop.io.Writable interface. </span></li><li><span style="color: rgb(0,0,0);line-height: 1.4285;">WritableResolver uses reflection to recreate the schema and populate its fields (for both read and write). Then it uses the Writable interface functions to read/write.<br></span><span style="color: rgb(0,0,0);line-height: 1.4285;">Therefore, fields must be public, to enable access to them. Private fields will be ignored.</span></li><li><span style="color: rgb(0,0,0);line-height: 1.4285;">Fields are accessed and populated by the order in which they are declared in the class file.</span></li><li><span style="color: rgb(0,0,0);line-height: 1.4285;">Supported field types:</span><br><span style="color: rgb(0,0,0);line-height: 1.4285;">String, int, double, float, long, short, boolean, byte array.<br></span><span style="color: rgb(0,0,0);line-height: 1.4285;">Arrays of any of the above types is supported, but the constructor must define the array size, so the reflection will work.</span></li></ol><h2 id="PXFInstallationandAdministration-AccessingHiveDatawithPXF">Accessing Hive Data with PXF</h2><h3 id="PXFInstallationandAdministration-InstallingthePXFHIVEplugin">Installing the PXF HIVE plugin</h3><p>Install the PXF HIVE plugin on all nodes in the cluster:<span style="line-height: 1.4285;font-size: 14.0px;background-color: transparent;">&nbsp;</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: bash; gutter: false" style="font-size:12px;">sudo rpm -i pxf-hive-2.3.0.0-x.rpm</pre>
</div></div><ul><li class="Code">PXF RPMs reside in the Pivotal ADS/HAWQ stack file.&nbsp;</li><li>The script installs the JAR file at the default location at&nbsp;<em>/usr/lib/gphd/pxf-2.3.0.0</em>. The S<em>oftlink pxf-hive.jar</em>&nbsp;will be created in&nbsp;<em>/usr/lib/gphd/pxf</em><br><em><br></em></li></ul> <div class="aui-message hint shadowed information-macro">
<p class="title">PXF HIVE Prerequisites</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p>Check the following before <span style="color: rgb(51,51,51);">adding PXF support on Hive:</span></p><ul><li><span style="color: rgb(51,51,51);">PXF HDFS plugin is installed on the cluster nodes.</span></li><li><span style="line-height: 1.4285715;">You are running the Hive Metastore service on a machine in your cluster.&nbsp;</span></li><li><span style="line-height: 1.4285715;">Check that you have set the </span><em style="line-height: 1.4285715;">hive.metastore.uris property</em><span style="line-height: 1.4285715;">&nbsp;in the&nbsp;</span><em style="line-height: 1.4285715;">hive-site.xml</em><span style="line-height: 1.4285715;"> on the Namenode.</span></li><li><span style="line-height: 1.4285715;">The Hive JAR files and conf directory&nbsp;are installed on the cluster nodes.</span></li></ul>
</div>
</div>
<h3 id="PXFInstallationandAdministration-Syntax.1">Syntax</h3><p>Hive tables are always defined in a specific way in PXF, regardless of the underlying file storage format. The PXF Hive plugins automatically detect source tables:</p><ul><li>Text based</li><li>SequenceFile</li><li>RCFile</li><li>ORCFile</li></ul><p>The source table can also be a combination of these types. The PXF Hive plugin&nbsp;uses this information to query the data in runtime. The following&nbsp;PXF table definition is valid for any file storage type.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE hivetest(id int, newid int)　
LOCATION ('pxf://&lt;NN host&gt;:50070/&lt;hive table name&gt;?PROFILE=Hive')
FORMAT 'custom' (formatter='pxfwritable_import');

SELECT * FROM hivetest;</pre>
</div></div><p><span style="line-height: 1.4285;">&nbsp;</span> <strong>Note</strong> <span>: 50070, as noted in the example above, is the REST server port on the HDFS NameNode. If a different port is assigned in your installation, use that port.</span></p><h3 id="PXFInstallationandAdministration-HiveCommandLine">Hive Command Line</h3><p><span style="line-height: 1.4285;">To start the Hive command line and work directly on a Hive table:</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: powershell; gutter: false" style="font-size:12px;">/&gt;${HIVE_HOME}/bin/hive</pre>
</div></div><p>Here's an example&nbsp;of how to create and load data into&nbsp;a sample Hive table from an existing file.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">Hive&gt; CREATE TABLE test (name string, type string, supplier_key int, full_price double) row format delimited fields terminated by ',';
Hive&gt; LOAD DATA local inpath '/local/path/data.txt' into table test;&nbsp;</pre>
</div></div><h3 id="PXFInstallationandAdministration-MappingHiveCollectionTypes"><span style="line-height: 1.5625;">Mapping Hive Collection Types</span></h3><p><span style="line-height: 1.5625;">PXF supports Hive data types that are not primitive types. For example</span> <span style="line-height: 1.4285;">:</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE TABLE sales_collections (
  item STRING,&nbsp;
  price FLOAT,&nbsp;
  properties ARRAY&lt;STRING&gt;,&nbsp;
  hash MAP&lt;STRING,FLOAT&gt;,&nbsp;
  delivery_address STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;
)  
ROW FORMAT DELIMITED  FIELDS
TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' LINES TERMINATED BY '\n'  STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '/local/path/&lt;some data file&gt;' INTO TABLE sales_collection;</pre>
</div></div><p><span style="color: rgb(51,51,51);line-height: 1.4285;font-size: 14.0px;">To query a Hive table schema similar to the one in the example, you need to define the PXF external table with attributes corresponding to members in the Hive table array and map fields. For example:</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE gp_sales_collections(
  item_name TEXT,
  item_price REAL,
  property_type TEXT,
  property_color TEXT,
  property_material TEXT,
  hash_key1 TEXT,
  hash_val1 REAL,
  hash_key2 TEXT,
  hash_val3 REAL,
  delivery_street TEXT,
  delivery_city TEXT,
  delivery_state TEXT,
  delivery_zip INTEGER
)
LOCATION ('pxf://&lt;namenode_host&gt;:50070/sales_collections?PROFILE=Hive')
FORMAT 'custom' (FORMATTER='pxfwritable_import');</pre>
</div></div><h3 id="PXFInstallationandAdministration-PartitionFiltering">Partition Filtering</h3><p align="LEFT">The PXF Hive plugin uses the Hive partitioning feature and directory structure. This enables partition exclusion on HDFS files that&nbsp;contain the Hive table.&nbsp;To use&nbsp;the Partition Filtering&nbsp;feature to reduce network traffic and I/O, run a PXF query using a WHERE clause&nbsp;that refers to a specific partition in the partitioned Hive table.</p><p align="LEFT">To take advantage of PXF Partition filtering push-down, name the partition fields in the external table. These names must be the same as those stored in the Hive table. Otherwise, PXF ignores Partition filtering and the filtering is performed on the HAWQ side, impacting&nbsp;performance.</p> <div class="aui-message hint shadowed information-macro">
<p class="title">NOTE</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p align="LEFT">The Hive plugin only filters on partition columns, not on other table attributes.</p>
</div>
</div>
<h4 id="PXFInstallationandAdministration-Example.1">Example</h4><p align="LEFT">Create a&nbsp;Hive tabl<em>e&nbsp;sales_part</em>&nbsp;with 2 partition columns - <em>delivery_state</em> and <em>delivery_city:</em></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE TABLE sales_part (name STRING, type STRING, supplier_key INT, price DOUBLE)
PARTITIONED BY (delivery_state STRING, delivery_city STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
</pre>
</div></div><p>Load data into this Hive table and&nbsp;add some partitions:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: plain; gutter: false" style="font-size:12px;">LOAD DATA LOCAL INPATH '/local/path/data1.txt' INTO TABLE sales_part PARTITION(delivery_state = 'CALIFORNIA', delivery_city = 'San Francisco');
LOAD DATA LOCAL INPATH '/local/path/data2.txt' INTO TABLE sales_part PARTITION(delivery_state = 'CALIFORNIA', delivery_city = 'Sacramento');
LOAD DATA LOCAL INPATH '/local/path/data3.txt' INTO TABLE sales_part PARTITION(delivery_state = 'NEVADA'    , delivery_city = 'Reno');
LOAD DATA LOCAL INPATH '/local/path/data4.txt' INTO TABLE sales_part PARTITION(delivery_state = 'NEVADA'    , delivery_city = 'Las Vegas');</pre>
</div></div><p align="LEFT">The Hive storage directory should appears as follows:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: plain; gutter: false" style="font-size:12px;">/hive/warehouse/sales_part/delivery_state=CALIFORNIA/delivery_city=’San Francisco’/data1.txt
/hive/warehouse/sales_part/delivery_state=CALIFORNIA/delivery_city=Sacramento/data2.txt
/hive/warehouse/sales_part/delivery_state=NEVADA/delivery_city=Reno/data3.txt
/hive/warehouse/sales_part/delivery_state=NEVADA/delivery_city=’Las Vegas’/data4.txt</pre>
</div></div><p>To define a PXF table to read this Hive table&nbsp;and&nbsp;take<strong> </strong>advantage of partition filter push-down, define the fields corresponding to the Hive partition fields at the end of the attribute list.&nbsp;</p><p>When defining an external table, check that&nbsp;the fields corresponding to the Hive partition fields are at the end of the column list. In HiveQL, issuing&nbsp;a&nbsp;<em>select*&nbsp;</em>&nbsp;statement on a partitioned table shows the partition fields at the end of the record.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE pxf_sales_part(
  item_name TEXT, 
  item_type TEXT, 
  supplier_key INTEGER, 
  item_price DOUBLE PRECISION, 
  delivery_state TEXT, 
  delivery_city TEXT
)
LOCATION ('pxf://namenode_host:50070/sales_part?Profile=Hive')
FORMAT 'custom' (FORMATTER='pxfwritable_import');

SELECT * FROM pxf_sales_part;</pre>
</div></div><h4 id="PXFInstallationandAdministration-Example.2">Example&nbsp;</h4><p><span style="line-height: 1.4285;">In the following example, the HAWQ query filters the <em>delivery_city</em> partition <em>Sacramento</em>. The filter on&nbsp;</span> <em style="line-height: 1.4285;">item_name</em> <span style="line-height: 1.4285;"> is not pushed down, since it is not a partition column. It is </span> <span style="line-height: 1.4285;">performed on the HAWQ side after all the data on <em>Sacramento</em> is transferred for </span> <span style="line-height: 1.4285;">processing.</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">SELECT * FROM pxf_sales_part WHERE delivery_city = 'Sacramento' AND item_name = 'shirt';</pre>
</div></div><h4 id="PXFInstallationandAdministration-Example.3"><span style="line-height: 1.4285;">Example</span></h4><p><span style="line-height: 1.4285;">The following HAWQ query reads all the data under&nbsp;<em>delivery_city</em> partition <em>CALIFORNIA</em>, regardless of the city partiti</span><span style="line-height: 1.4285;">on.</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">SELECT * FROM pxf_sales_part WHERE delivery_state = 'CALIFORNIA'</pre>
</div></div><h2 id="PXFInstallationandAdministration-AccessingHBaseDatawithPXF">Accessing HBase Data with PXF</h2><h3 id="PXFInstallationandAdministration-InstallingthePXFHBaseplugin">Installing the PXF HBase plugin</h3><p>Install the PXF HBase plugin on all nodes in the cluster:<span>&nbsp;</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: bash; gutter: false" style="font-size:12px;">sudo rpm -i pxf-hbase-2.3.0.0-x.rpm</pre>
</div></div><ul><li class="Code">PXF RPMs reside in the Pivotal ADS/HAWQ stack file.&nbsp;</li><li>The script installs the JAR file at the default location at&nbsp;<em>/usr/lib/gphd/pxf-2.3.0.0</em>. The S<em>oftlink pxf-hbase.jar</em>&nbsp;will be created in&nbsp;<em>/usr/lib/gphd/pxf</em><br><em><br></em></li></ul> <div class="aui-message hint shadowed information-macro">
<p class="title">PXF HBase Prerequisites</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p><span>Before using the PXF HBase plugin, verify the following:</span></p><ul><li><span>PXF HDFS plugin is installed on the cluster nodes.</span></li><li><span>HBase and zookeeper jars are installed on the cluster nodes.</span></li><li>HBase conf directory is updated on the cluster nodes.</li></ul>
</div>
</div>
<h3 id="PXFInstallationandAdministration-Syntax.2">Syntax</h3><p><span style="color: rgb(51,51,51);line-height: 1.4285;font-size: 14.0px;">&nbsp;</span> <span style="color: rgb(51,51,51);line-height: 1.4285;font-size: 14.0px;">To query an <em>HBase</em> table, use the following syntax:</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE &lt;pxf tblname&gt; (&lt;col list - see details below&gt;)
LOCATION ('pxf://&lt;NN REST host&gt;:&lt;NN REST port&gt;/&lt;HBase table name&gt;?PROFILE=HBase') 
FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
&nbsp;
SELECT * FROM &lt;pxf tblname&gt;;</pre>
</div></div><h2 id="PXFInstallationandAdministration-ColumnMapping">Column Mapping</h2><p>Most HAWQ external tables (PXF or others) require that the HAWQ table attributes match the source data record layout, and include all&nbsp;the available attributes. However, use the PXF HBase plugin to&nbsp;specify the subset of HBase qualifiers that define the HAWQ PXF table.&nbsp;To set up a&nbsp;clear mapping between each attribute in the PXF table and a specific qualifier in the HBase table, you can use either:</p><ul><li>Direct mapping</li><li>Indirect mapping</li></ul><p>In addition, the HBase row key is handled in a special way.</p><h2 id="PXFInstallationandAdministration-RowKey">Row Key</h2><p>You can use the HBase table row key in several ways. For example,&nbsp;you can see them using query results,&nbsp;or&nbsp;you can run a&nbsp;WHERE clause filter on a range of row key values. To use the row key in the HAWQ query, define the HAWQ table with the reserved PXF attribute&nbsp;<em>recordkey.</em>&nbsp;This attribute name tells PXF to return the&nbsp;record key in any key-value based system and in HBase.</p> <div class="aui-message hint shadowed information-macro">
<p class="title">NOTE</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p>Since HBase is byte and not character-based, Pivotal recommends that you define the <em>recordkey</em> as type <em>bytea</em>. This may result in better ability to filter data and increase performance.</p>
</div>
</div>
<div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE &lt;tname&gt; (recordkey bytea, ... ) LOCATION ('pxf:// ...')</pre>
</div></div><h3 id="PXFInstallationandAdministration-DirectMapping">Direct Mapping</h3><p>Use&nbsp;Direct Mapping&nbsp;to map HAWQ table attributes to HBase qualifiers. You can specify the HBase qualifier names of interest, with column family names included, as quoted values.&nbsp;</p><p>For example, you have defined an <span>HBase table called&nbsp;<em>hbase_sales</em> with multiple column families and many qualifiers. To see the following in the resulting attribute section of the CREATE EXTERNAL TABLE:</span></p><ul><li><span><em>rowkey</em></span></li><li><span>qualifier <em>saleid</em> in the&nbsp;column family <em>cf1</em></span></li><li><span>qualifier<em> comments</em> in the&nbsp;column family<em> cf8</em>&nbsp;</span></li></ul><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE hbase_sales (
  recordkey bytea,
  “cf1:saleid” int,
  “cf8:comments” varchar
) ...</pre>
</div></div><p>The PXF HBase plugin uses these attribute names as-is and returns the values of these HBase qualifiers.</p><h3 id="PXFInstallationandAdministration-IndirectMapping(viaLookupTable)">Indirect Mapping (via Lookup Table)</h3><p>Direct mapping method is fast and intuitive, but using&nbsp;indirect mapping&nbsp;helps to&nbsp;reconcile HBase qualifier names with HAWQ behavior:</p><ul><li>HBase qualifier names that are longer than 32 characters. HAWQ has a 32 character limit on attribute name size.</li><li>HBase qualifier names&nbsp;can be binary or non-printable. HAWQ attribute names are character based.</li></ul><p>In&nbsp;either case, Indirect Mapping uses a lookup table on HBase. You can create the lookup table to store all necessary lookup information.&nbsp;This works as a template for any future queries. The name of the lookup table must be&nbsp;<em>pxflookup&nbsp;</em>and must include&nbsp;the column family named&nbsp;<em>mapping.&nbsp;</em></p><p>Using the sales example in Direct Mapping, if our<em> rowkey</em> represents the HBase table name and the&nbsp;<em>mapping&nbsp;</em>column family includes the actual attribute mapping in the key value form of <em>&lt;hawq attr name&gt;=&lt;hbase cf:qualifier&gt;. </em></p><h4 id="PXFInstallationandAdministration-Example.4">Example</h4><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">(row key)</th><th class="confluenceTh">mapping</th></tr><tr><td class="confluenceTd">sales</td><td class="confluenceTd">id=cf1:saleid</td></tr><tr><td class="confluenceTd">sales</td><td class="confluenceTd">cmts=cf8:comments</td></tr></tbody></table></div><p><strong>Note</strong>: The mapping assigned new names for each qualifier.&nbsp;You can use these names in your HAWQ table definition:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE hbase_sales (
  recordkey bytea
  id int,
  cmts varchar
) ...</pre>
</div></div><p>PXF automatically matches HAWQ to HBase column names when a&nbsp;<em>pxflookup&nbsp;</em>table exists in HBase.</p><h2 id="PXFInstallationandAdministration-AccessingGemFireXDDatawithPXF">Accessing GemFire XD Data with PXF</h2> <div class="aui-message hint shadowed information-macro">
<p class="title">NOTE</p>
<span class="aui-icon icon-hint">Icon</span>
<div class="message-content">
<p><span>Before using PXF GemFire XD plugin, verify the following:</span></p><p><span>&nbsp; - That you have installed the&nbsp;</span> <em>gfxd rpm </em> <span>&nbsp;on the Namenode and on the Datanodes.</span></p><p><span>&nbsp; - The Namenode and all Datanodes have the&nbsp;<em>gemfirexd.jar</em> set in <em>pxf-public.classpath</em> file</span></p><p><span>See <em><a class="active" href="PXFInstallationandAdministration.html">Installing PXF</a></em> for more information.</span></p>
</div>
</div>
<h3 id="PXFInstallationandAdministration-Syntax.3">Syntax</h3><p><span style="color: rgb(51,51,51);line-height: 1.4285;font-size: 14.0px;">&nbsp;</span> <span style="color: rgb(51,51,51);line-height: 1.4285;font-size: 14.0px;">To query an <em>GemFire</em>&nbsp;<em>XD</em> table use the following syntax:</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: sql; gutter: false" style="font-size:12px;">CREATE EXTERNAL TABLE &lt;pxf tblname&gt; (&lt;col list&gt;)
LOCATION ('pxf://&lt;NN REST host&gt;:&lt;NN REST port&gt;/&lt;GemFireXD table name&gt;?Profile=GemFireXD&amp;&lt;GemFireXD specific connector options&gt;') 
FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
&nbsp;
SELECT * FROM &lt;pxf tblname&gt;;</pre>
</div></div><p>The GemFire XD connector has quite a few connector options that can be used in the LOCATION URI, and are well documented in the GemFire XD document itself.</p><p>It is highly recommended to learn them carefully in order to get the expected behavior when querying GemFire XD data through PXF.</p><h2 id="PXFInstallationandAdministration-Troubleshooting">Troubleshooting</h2><p>The following table describes some common errors while using PXF:</p><p><strong>Table: PXF Errors and Explanation</strong></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><td class="confluenceTd"><p align="LEFT"><strong>Error</strong></p></td><td class="confluenceTd"><p align="LEFT"><strong>Common Explanation</strong></p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; invalid URI pxf://localhost:50070/demo/file1: missing options section</em></p></td><td class="confluenceTd"><p align="LEFT"><em>LOCATION</em> does not include options after the file name: <em>&lt;path&gt;?&lt;key&gt;=&lt;value&gt;&amp;&lt;key&gt;=&lt;value&gt;...</em></p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; protocol "pxf" does not exist</em></p></td><td class="confluenceTd"><p align="LEFT">HAWQ is not compiled with PXF<span style="font-size: xx-small;">&nbsp;</span> protocol. It requires&nbsp;the GPSQL&nbsp;version of&nbsp;HAWQ version.</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; remote component error (0) from '&lt;x&gt;':</em><span style="line-height: 1.4285715;"> There is no pxf servlet listening on the host and port specified in the external table url.</span></p></td><td class="confluenceTd"><p align="LEFT">Wrong server or port or the service is not started.</p><p align="LEFT">&nbsp;</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; Missing FRAGMENTER option in the pxf uri: pxf://localhost:50070/demo/file1?a=a</em></p></td><td class="confluenceTd"><p align="LEFT">No <em>FRAGMENTER</em> option was specified in <em>LOCATION</em>.</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;': &nbsp; type &nbsp;Exception report &nbsp; message &nbsp; org.apache.hadoop.mapred.InvalidInputException:</em><em style="line-height: 1.4285;"> Input path does not exist: hdfs://0.0.0.0:8020/demo/file1&nbsp;</em><span style="line-height: 1.4285;">&nbsp;</span></p></td><td class="confluenceTd"><p align="LEFT">File or pattern given in<em> LOCATION</em> doesn't exist on specified path.&nbsp;</p></td></tr><tr><td class="confluenceTd"><p align="LEFT">ERROR:&nbsp;<em> remote component error (500) from '&lt;x&gt;': &nbsp; type &nbsp;Exception report &nbsp; message &nbsp; org.apache.hadoop.mapred.InvalidInputException</em><em style="line-height: 1.4285;">: Input Pattern hdfs://0.0.0.0:8020/demo/file*</em><span style="line-height: 1.4285;"> matches 0 files&nbsp;</span></p></td><td class="confluenceTd"><p align="LEFT">File or pattern given in <em>LOCATION</em> doesn't exist on specified path.</p><p align="LEFT">&nbsp;</p></td></tr><tr><td class="confluenceTd" colspan="1"><em>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;': <span style="color: rgb(0,0,0);">PXF not correctly installed in CLASSPATH</span> </em></td><td class="confluenceTd" colspan="1">Cannot find PXF Jar</td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; GPHD component not found</em></p></td><td class="confluenceTd"><p align="LEFT">Either the required data node does not exist or PXF service (tcServer) on data node is not started or PXF webapp was not started.</p></td></tr><tr><td class="confluenceTd" colspan="1">ERROR: &nbsp;remote component error (500) from '&lt;x&gt;': &nbsp;type &nbsp;Exception report &nbsp; message &nbsp; java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/client/HTableInterface</td><td class="confluenceTd" colspan="1">One of the classes required for running PXF or one of its plugins is missing. Check that all resources in the PXF classpath files exist on the cluster nodes.</td></tr><tr><td class="confluenceTd" colspan="1"><pre>ERROR:  remote component error (500)&nbsp;from '&lt;x&gt;': &nbsp; type &nbsp;Exception report &nbsp; message &nbsp; <span style="white-space: pre-wrap;font-family: Arial , sans-serif;line-height: 1.4285715;">java.io.IOException: Can't get Master Kerberos principal for use as renewer</span></pre></td><td class="confluenceTd" colspan="1">Secure PXF: YARN isn't properly configured for secure (Kerberized) HDFS installs</td></tr><tr><td class="confluenceTd" colspan="1"><pre>ERROR:  fail to get filesystem credential for uri hdfs://&lt;namenode&gt;:8020/</pre></td><td class="confluenceTd" colspan="1">Secure PXF: Wrong HDFS host or port is not 8020 (see <em><a class="active" href="PXFInstallationandAdministration.html">Limitations of Secure PXF</a></em>)</td></tr><tr><td class="confluenceTd" colspan="1"><pre>ERROR: remote component error (413) from '&lt;x&gt;': HTTP status code is 413 but HTTP response string is empty</pre></td><td class="confluenceTd" colspan="1">The PXF table number of attributes and their name sizes are too large for tcServer to accommodate in its request buffer. The solution is to increase the value of the maxHeaderCount and maxHttpHeaderSize<span style="color: rgb(0,0,0);"> parameters on </span><span style="color: rgb(0,0,0);">server.xml on tcServer instance on all nodes and then restart PXF:</span><br><p>&lt;Connector acceptCount="100"<br> connectionTimeout="20000"<br> executor="tomcatThreadPool"<br> maxKeepAliveRequests="15"<br>maxHeaderCount="&lt;some larger value&gt;"<br>maxHttpHeaderSize="&lt;some larger value in bytes&gt;"<br> port="${bio.http.port}"<br> protocol="org.apache.coyote.http11.Http11Protocol"<br> redirectPort="${bio.https.port}"/&gt;</p></td></tr><tr><td class="confluenceTd" colspan="2"><p align="LEFT"><strong>HBase Specific Errors</strong></p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;': &nbsp; type &nbsp;Exception report &nbsp; message &nbsp; </em><span style="line-height: 1.4285715;">&nbsp;</span><em style="line-height: 1.4285715;">org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for t1,,99999999999999</em><span style="line-height: 1.4285715;"> after 10 tries.</span></p></td><td class="confluenceTd"><p align="LEFT">HBase service is down, probably HRegionServer.</p><p align="LEFT">&nbsp;</p><p align="LEFT">&nbsp;</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;':&nbsp; type &nbsp;Exception report &nbsp; message &nbsp; </em><em style="line-height: 1.4285715;">org.apache.hadoop.hbase.TableNotFoundException:</em><span style="line-height: 1.4285715;"> nosuch</span></p></td><td class="confluenceTd"><p align="LEFT">HBase cannot find the requested table</p><p align="LEFT">&nbsp;</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><span>ERROR: &nbsp;remote component error (500) from '&lt;x&gt;': &nbsp;type &nbsp;Exception report &nbsp; message &nbsp; java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/client/HTableInterface</span></p></td><td class="confluenceTd"><p align="LEFT">PXF cannot find a required JAR file, probably HBase's</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;': &nbsp; type &nbsp;Exception report &nbsp; message &nbsp; java.lang.NoClassDefFoundError: org/apache/zookeeper/KeeperException</em></p></td><td class="confluenceTd"><p align="LEFT">PXF cannot find Zookeeper's JAR</p></td></tr><tr><td class="confluenceTd"><p align="LEFT">ERROR: &nbsp;remote component error (500) from '&lt;x&gt;': &nbsp;type &nbsp;Exception report &nbsp; message &nbsp; java.lang.Exception: java.lang.IllegalArgumentException: Illegal HBase column name a, missing :</p></td><td class="confluenceTd"><p align="LEFT">PXF table has an illegal field name. Each field name must correspond to an HBase column in the syntax &lt;column family&gt;:&lt;field name&gt;</p></td></tr><tr><td class="confluenceTd" colspan="1">ERROR: &nbsp;remote component error (500) from '&lt;x&gt;': &nbsp;type &nbsp;Exception report &nbsp; message &nbsp; org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family a does not exist in region t1,,1405517248353.85f4977bfa88f4d54211cb8ac0f4e644. in table 't1', {NAME =&amp;gt; 'cf', DATA_BLOCK_ENCODING =&amp;gt; 'NONE', BLOOMFILTER =&amp;gt; 'ROW', REPLICATION_SCOPE =&amp;gt; '0', COMPRESSION =&amp;gt; 'NONE', VERSIONS =&amp;gt; '1', TTL =&amp;gt; '2147483647', MIN_VERSIONS =&amp;gt; '0', KEEP_DELETED_CELLS =&amp;gt; 'false', BLOCKSIZE =&amp;gt; '65536', ENCODE_ON_DISK =&amp;gt; 'true', IN_MEMORY =&amp;gt; 'false', BLOCKCACHE =&amp;gt; 'true'}</td><td class="confluenceTd" colspan="1"><span>Required HBase table does not contain the requested column.</span></td></tr><tr><td class="confluenceTd" colspan="2"><p align="LEFT"><strong>Hive Specific Errors</strong></p></td></tr><tr><td class="confluenceTd"><p>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;': &nbsp;type &nbsp;Exception report &nbsp; message &nbsp;&nbsp;java.lang.RuntimeException: Failed to connect to Hive <span style="text-decoration: underline;">metastore</span>: java.net.ConnectException: Connection refused</p></td><td class="confluenceTd"><p align="LEFT">Hive Metastore service is down.</p><p align="LEFT">&nbsp;</p></td></tr><tr><td class="confluenceTd"><p align="LEFT"><em>ERROR:&nbsp; remote component error (500) from '&lt;x&gt;':&nbsp;type &nbsp;Exception report &nbsp; message </em><em style="line-height: 1.4285715;">NoSuchObjectException(<a rel="nofollow">message:default.players</a> table not found)</em></p></td><td class="confluenceTd"><p align="LEFT">Table doesn't exist in Hive.</p></td></tr><tr><td class="confluenceTd" colspan="2"><strong>GemfireXD Specific Errors</strong></td></tr><tr><td class="confluenceTd" colspan="1">No data or wrong data comes back, comes back with very poor performance,</td><td class="confluenceTd" colspan="1">See GemFireXD connector documentation as part of the GemFireXF product document. There are various GemFireXD connector options that need to be used properly in order to get the right results with good performance.</td></tr></tbody></table></div><p><span style="font-size: xx-small;"> <span style="font-size: small;">&nbsp;</span> </span></p><div><span style="color: rgb(0,0,0);line-height: 30.0px;font-size: 24.0px;"> <br> </span></div><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
</div>
		
		</main>

  </div>

  <script type="text/javascript">
    (function() {
      var didInit = false;
      function initMunchkin() {
        if(didInit === false) {
          didInit = true;
          Munchkin.init('625-IUJ-009');
        }
      }
      var s = document.createElement('script');
      s.type = 'text/javascript';
      s.async = true;
      s.src = document.location.protocol + '//munchkin.marketo.net/munchkin.js';
      s.onreadystatechange = function() {
        if (this.readyState == 'complete' || this.readyState == 'loaded') {
          initMunchkin();
        }
      };
      s.onload = initMunchkin;
      document.getElementsByTagName('head')[0].appendChild(s);
    })();
  </script>
</div>
</div>

<div id="scrim"></div>

<div class="container">
  <footer class="site-footer-links">
    <div class="copyright">
      <a href="/">Pivotal Documentation</a>
      © 2014 <a href="http://pivotal.io">Pivotal Software</a> Inc. All Rights Reserved.
  </div>
  <div class="support">
    Need help? <a href="http://support.pivotal.io" target="_blank">Visit support</a></div>
  </footer>
</div>

</body>
</html>